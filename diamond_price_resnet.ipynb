{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "APEhaTd9U4Ia"
      ],
      "authorship_tag": "ABX9TyMG+lkriZX0QsUTZZSWkW6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winengewe/diamond-price-resnet/blob/main/diamond_price_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# diamond-price-resnet"
      ],
      "metadata": {
        "id": "BreXIt_dZh1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A production-ready Deep Learning pipeline for diamond valuation using a custom ResNet-MLP architecture and Log-Norm target engineering."
      ],
      "metadata": {
        "id": "rJnYnj95DnLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# 1. IMPORTS (Getting our tools)\n",
        "# ==========================================\n",
        "import pandas as pd             # For handling tables of data (like Excel)\n",
        "import numpy as np              # For heavy math operations\n",
        "import matplotlib.pyplot as plt # For drawing graphs\n",
        "import seaborn as sns           # For loading the diamond dataset\n",
        "import pickle                   # For saving our work to use later\n",
        "import os                       # For talking to the operating system\n",
        "\n",
        "# Machine Learning tools from Scikit-Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer # <--- NEW TOOL for Fix B\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Deep Learning tools from TensorFlow/Keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks, optimizers, losses, metrics\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "TIMESERIES = 'No' # 'Yes' or 'No' for tabular regression\n",
        "PROBLEM_TYPE = 'Regression' # 'Regression' or 'Classification'\n",
        "OUTPUTS_NO = 1\n",
        "# 1 for regression predicted value\n",
        "# 2 for classification either 'fraud' or 'no fraud' / 'good' or 'bad'\n",
        "# 10 for MNIST-1D\n",
        "\n",
        "# --- MASTER SWITCH ---\n",
        "# Set to False to use the fast settings below\n",
        "USE_DEFAULTS = False\n",
        "\n",
        "if USE_DEFAULTS:\n",
        "    print(\"Using DEFAULT configuration.\")\n",
        "    EPOCHS = 50\n",
        "    BATCH_SIZE = 32\n",
        "    HIDDEN_LAYERS = 3\n",
        "    Initial_LR = 0.001\n",
        "    BASE_NEURONS = 64\n",
        "else:\n",
        "    print(\"Using CUSTOM (Fast Track) configuration.\")\n",
        "    EPOCHS = 5           # Reduced from 50 (Train for less time)\n",
        "    BATCH_SIZE = 128     # Increased from 32 (Process more data at once)\n",
        "    HIDDEN_LAYERS = 2    # Slightly smaller brain for speed\n",
        "    Initial_LR = 0.001\n",
        "    BASE_NEURONS = 64\n",
        "\n",
        "# Settings to make sure get the same results every run / Ensure reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "keras.utils.set_random_seed(RANDOM_SEED)\n",
        "\n",
        "# ==========================================\n",
        "# 2. PREPARING THE DATA\n",
        "# ==========================================\n",
        "def load_and_process_data():\n",
        "    print(\"--- 1. Loading and Cleaning Data ---\")\n",
        "    # Load the dataset (built into the seaborn library)\n",
        "    df = sns.load_dataset('diamonds') # Pandas uses objects/strings and float64 by default\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "    # Remove any rows that have missing information (blank cells)\n",
        "    # print(\"\\nMissing values:\\n\", df.isnull().sum()) # Show the total of missing values for each column\n",
        "    df = df.dropna()\n",
        "    # print(\"\\nAfter dropping rows with missing values:\\n\", df.isnull().sum())\n",
        "    # print(f\"\\nCleaned shape: {df.shape}\")\n",
        "\n",
        "    # --- ENHANCEMENT 1: Correlation Matrix ---\n",
        "    # We need to turn text columns into temporary numbers just for the heatmap\n",
        "    print(\"\\nGenerating Correlation Matrix...\")\n",
        "    df_analysis = df.copy()\n",
        "    le = LabelEncoder()\n",
        "    for col in ['cut', 'color', 'clarity']:\n",
        "        df_analysis[col] = le.fit_transform(df_analysis[col])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Calculate how much every column correlates with every other column\n",
        "    corr = df_analysis.corr()\n",
        "    # Draw the heatmap\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation Matrix (What drives Price?)\")\n",
        "    plt.show()\n",
        "    print(\"Note: 'Carat' and dimensions (x, y, z) have the highest correlation with Price.\\n\")\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Separate the answers (Price) from the questions (Features)\n",
        "    X = df.drop(columns=['price']) # The features (extra features: ['Class','Value'])\n",
        "    y = df['price'] # The target (Price)\n",
        "\n",
        "    # --- Concept: Train/Test Split ---\n",
        "    # We hide 20% of the data (Test set) to quiz the model later.\n",
        "    # We only train on the other 80%.\n",
        "    # Instead of just Train/Test, split into Train/Validation/Test\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Split the full train into Train and Val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "    # --- FIX A: Log Transformation for Price ---\n",
        "    # Instead of StandardScaling the price, we use Logarithmic scaling.\n",
        "    # np.log1p calculates log(1 + x) to handle zero values safely.\n",
        "    # np.log1p(y) is usually all the scaling you need for financial data, and why adding StandardScaler on top of it is often redundant.\n",
        "    print(\"Applying Log Transform to Target...\")\n",
        "    y_train = np.log1p(y_train)\n",
        "    y_val = np.log1p(y_val)\n",
        "    y_test = np.log1p(y_test)\n",
        "\n",
        "    # --- FIX B: Column Transformer (The Pro Way) ---\n",
        "    # We define a \"processor\" that treats numerical and categorical columns differently\n",
        "    numerical_cols = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "    categorical_cols = ['cut', 'color', 'clarity']\n",
        "\n",
        "    # 1. Pipeline for Numbers: Just Scale them\n",
        "    # 2. Pipeline for Categories: One Hot Encode them\n",
        "    #    handle_unknown='ignore' ensures the code won't crash if a new weird category appears later.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_cols),\n",
        "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Fit ONLY on Training data\n",
        "    # Fit preprocessor ONLY on the smaller X_train\n",
        "    print(\"Fitting Preprocessor on Train Data...\")\n",
        "    X_train = preprocessor.fit_transform(X_train)\n",
        "    X_val = preprocessor.transform(X_val)   # Just transform\n",
        "    X_test = preprocessor.transform(X_test) # Just transform\n",
        "\n",
        "    # FINAL CONVERSION (float32 for training.)\n",
        "    # NOW we convert everything to the format the Neural Network wants.\n",
        "\n",
        "    # 1. Convert X to float32 (The preprocessor usually outputs float64)\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_val = X_val.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    # 2. Convert y to float32 and Reshape\n",
        "    # .values turns the Pandas Series into a Numpy Array\n",
        "    # .reshape(-1, 1) makes it a column vector (needed for Keras)\n",
        "    y_train = y_train.astype('float32').values.reshape(-1, 1)\n",
        "    y_val = y_val.astype('float32').values.reshape(-1, 1)\n",
        "    y_test = y_test.astype('float32').values.reshape(-1, 1)\n",
        "\n",
        "    print(f\"Processed Train Shape X: {X_train.shape}\")\n",
        "    print(f\"Processed Val Shape X: {X_val.shape}\")\n",
        "    print(f\"Processed Test Shape X: {X_test.shape}\")\n",
        "    print(f\"Processed Train Shape y: {y_train.shape}\")\n",
        "    print(f\"Processed Val Shape y: {y_val.shape}\")\n",
        "    print(f\"Processed Test Shape y: {y_test.shape}\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, preprocessor\n",
        "\n",
        "# Run the function\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, preprocessor = load_and_process_data()\n",
        "\n",
        "# ========================================================\n",
        "# 3. BUILDING THE BRAIN (Model Architecture: (ResNet-MLP))\n",
        "# ========================================================\n",
        "def build_neural_network(input_shape, hiddenlayers=3, base_neurons=64, outputs_no=1, problem_type='Regression'):\n",
        "    # This is the entry door for data\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: A simple dense layer / Initial Linear Projection\n",
        "    z = layers.Dense(base_neurons * hiddenlayers)(inputs) # for 'keras.layers.Add' same size with s (AI class)\n",
        "    z = layers.Dropout(0.1)(z) # 'Dropout' randomly ignores 10% of neurons to prevent memorization\n",
        "\n",
        "    # Layer 2: A complex block (ResNet style)\n",
        "    # Residual Blocks with Inverted Bottleneck\n",
        "    # (Similar to Transformer Feedforward Network (FFN): Expand -> Act -> Contract)\n",
        "    for _ in range(hiddenlayers):\n",
        "      shortcut = z\n",
        "      z = layers.LayerNormalization()(z)  # Stabilizes the math / deep networks\n",
        "      z = layers.Dense(4* base_neurons * hiddenlayers, activation='gelu')(z) # Expand to think about complex features (AI class)\n",
        "      z = layers.Dense(base_neurons * hiddenlayers)(z) # Contract back down (AI class)\n",
        "      z = layers.Dropout(0.1)(z) # Dropout on linear projection\n",
        "      z = layers.Add()([shortcut, z])  # Add the original info back (Skip Connection)\n",
        "\n",
        "    # Output Layer: One single number (The predicted price)\n",
        "    z = layers.LayerNormalization()(z)\n",
        "\n",
        "    # Output Logic\n",
        "    if problem_type == 'Regression':\n",
        "      activation = 'linear' # 'linear' for regression (default) ranges from $-\\infty$ to $+\\infty$\n",
        "      # activation = 'gelu' # 'gelu' for regression (AI class) output -0.17 to Infinity\n",
        "      loss_fn = losses.MeanSquaredError() # Penalize big errors heavily\n",
        "      metric_list = [metrics.MeanAbsoluteError(name='mae')] # A human-readable error score\n",
        "    elif problem_type == 'Classification':\n",
        "      activation = 'sigmoid' if outputs_no == 1 else 'softmax'\n",
        "      loss_fn = losses.SparseCategoricalCrossentropy() # (AI class)\n",
        "      metric_list = [metrics.SparseCategoricalAccuracy()] # (AI class)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown problem type: {problem_type}\")\n",
        "\n",
        "    outputs = layers.Dense(outputs_no, activation=activation)(z)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Diamond_ResNet\")\n",
        "\n",
        "    return model, loss_fn, metric_list\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAINING THE MODEL\n",
        "# ==========================================\n",
        "print(\"\\n--- 2. Training the Model ---\")\n",
        "\n",
        "model, loss_fn, metric_list = build_neural_network(input_shape=(X_train.shape[1],),\n",
        "hiddenlayers=HIDDEN_LAYERS, base_neurons=BASE_NEURONS,\n",
        "outputs_no=OUTPUTS_NO, problem_type=PROBLEM_TYPE)\n",
        "\n",
        "# Learning Rate Schedule (Cosine Decay)\n",
        "# Smoothly lowers LR from Initial_LR to 0 over the course of training\n",
        "total_steps = int(np.ceil(len(X_train) / BATCH_SIZE) * EPOCHS)\n",
        "lr_schedule = optimizers.schedules.CosineDecay(initial_learning_rate=Initial_LR,decay_steps=total_steps)\n",
        "\n",
        "# Compile tells the model how to measure its own errors\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule),loss=loss_fn,metrics=metric_list)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Safety Nets\n",
        "# EarlyStopping: Stop training if the model stops improving (saves time)\n",
        "# ModelCheckpoint: Always save the best version of the model, not just the last one\n",
        "callbacks_list = [\n",
        "    callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "    callbacks.ModelCheckpoint('best_diamond_model.keras', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "# The .fit() command is where the actual learning happens\n",
        "# Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val), # Pass the explicit data instead of 'split'\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =======================================================\n",
        "# 5. MODEL TESTING (EVALUATION & VISUALIZATION) & SAVING\n",
        "# =======================================================\n",
        "print(\"\\n--- 3. Evaluation & Saving ---\")\n",
        "\n",
        "# 1. Generate Predictions\n",
        "# The model predicts in \"Log Scale\", so we must convert back to Dollars using np.expm1\n",
        "y_pred_log = model.predict(X_test, verbose=0)\n",
        "y_test_dollars = np.expm1(y_test)\n",
        "y_pred_dollars = np.expm1(y_pred_log)\n",
        "residuals = y_pred_dollars - y_test_dollars\n",
        "\n",
        "print(f\"Mean Residual: ${np.mean(residuals):,.2f}\")\n",
        "print(f\"Median Residual: ${np.median(residuals):,.2f}\")\n",
        "print(f\"Std of Residuals: ${np.std(residuals):,.2f}\")\n",
        "\n",
        "mae_dollars = np.mean(np.abs(residuals))\n",
        "mape = np.mean(np.abs(residuals / y_test_dollars)) * 100\n",
        "print(f\"Test MAE (Dollars): ${mae_dollars:,.2f}\")\n",
        "print(f\"Test MAPE: {mape:.2f}%\")\n",
        "\n",
        "# 2. Calculate Metrics\n",
        "# R2 Score = How well the trend matches (1.0 is perfect)\n",
        "# MAE = Average error in log units\n",
        "loss, mae_log = model.evaluate(X_test, y_test, verbose=0)\n",
        "r2 = r2_score(y_test_dollars, y_pred_dollars)\n",
        "\n",
        "print(f\"Final Test MAE (Log scale): {mae_log:.4f}\")\n",
        "print(f\"R² Score (closer to 1 is better): {r2:.4f}\")\n",
        "\n",
        "# 3. Save Artifacts\n",
        "#    We save the 'preprocessor' (Scaling/Encoding) so we can process new data later\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "print(\"Artifacts (preprocessor.pkl) saved successfully.\")\n",
        "\n",
        "#    (Optional) Save the final model explicitly\n",
        "model.save('final_diamond_model.keras')\n",
        "print(\"Final Model (final_diamond_model.keras) saved successfully.\")\n",
        "\n",
        "# 4. Visualization Dashboard\n",
        "print(\"\\nGenerating Dashboard...\")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# --- PLOT 1: LOSS (MSE) ---\n",
        "axes[0].plot(history.history['loss'], label='Train Loss')\n",
        "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
        "axes[0].set_title('Learning Curve (Loss/MSE)')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('MSE (Log Scale)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# --- PLOT 2: ERROR (MAE) ---\n",
        "axes[1].plot(history.history['mae'], label='Train MAE')\n",
        "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
        "axes[1].set_title('Absolute Error (MAE)')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# --- PLOT 3: ACTUAL VS PREDICTED ---\n",
        "# Scatter points\n",
        "axes[2].scatter(y_test_dollars, y_pred_dollars, alpha=0.3, color='blue', label='Diamonds')\n",
        "\n",
        "# The \"Perfect Prediction\" Line\n",
        "# Ensure they are simple 1D arrays for plotting\n",
        "y_test_flat = y_test_dollars.ravel()\n",
        "min_val = 0\n",
        "# Cap the plot at 99th percentile to avoid scaling issues if there's a crazy outlier\n",
        "max_val = np.percentile(y_test_flat, 99.5) * 1.1\n",
        "\n",
        "axes[2].plot([min_val, max_val], [min_val, max_val], color='red', linewidth=2, linestyle='--', label='Perfect Fit')\n",
        "axes[2].set_title(f'Prediction Accuracy (R²: {r2:.4f}) closer to 1 is better')\n",
        "axes[2].set_xlabel('Actual Price ($)')\n",
        "axes[2].set_ylabel('Predicted Price ($)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 6. USING THE MODEL (Prediction)\n",
        "# ==========================================\n",
        "print(\"\\n--- 4. Prediction Demo ---\")\n",
        "\n",
        "# Load artifacts once\n",
        "loaded_model = keras.models.load_model('best_diamond_model.keras')\n",
        "with open('preprocessor.pkl', 'rb') as f:\n",
        "    loaded_preprocessor = pickle.load(f)\n",
        "\n",
        "def predict_diamond_price(new_data_dict, model, preprocessor):\n",
        "    \"\"\"\n",
        "    Accepts raw features, processes via ColumnTransformer,\n",
        "    predicts Log-Price, and converts back to Dollars.\n",
        "    \"\"\"\n",
        "    # 1. Convert the dictionary to a DataFrame Table\n",
        "    input_df = pd.DataFrame([new_data_dict])\n",
        "\n",
        "    # 2. Process using the pipeline (Scales numbers & Encodes text automatically)\n",
        "    #    (This handles the OneHotEncoding alignment automatically!)\n",
        "    input_processed = preprocessor.transform(input_df)\n",
        "\n",
        "    # 3. Predict (Result is in Log Scale)\n",
        "    prediction_log = model.predict(input_processed, verbose=0)\n",
        "\n",
        "    # 4. Inverse Log (Convert back to Real Dollars)\n",
        "    #    np.expm1 is the inverse of np.log1p\n",
        "    prediction_dollars = np.expm1(prediction_log)\n",
        "\n",
        "    return prediction_dollars[0][0]\n",
        "\n",
        "\n",
        "# --- Demo ---\n",
        "# Let's pretend we have a new diamond to sell\n",
        "\n",
        "\"\"\"\n",
        "sample_diamond = {\n",
        "    'carat': 1.5,\n",
        "    'cut': 'Ideal',\n",
        "    'color': 'G',\n",
        "    'clarity': 'VVS2',\n",
        "    'depth': 61.5,\n",
        "    'table': 55.0,\n",
        "    'x': 7.0, 'y': 7.0, 'z': 4.3\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "sample_diamond = { #18818\n",
        "    'carat': 1.21,\n",
        "    'cut': 'Very Good',\n",
        "    'color': 'E',\n",
        "    'clarity': 'SI1',\n",
        "    'depth': 62.2,\n",
        "    'table': 60,\n",
        "    'x': 6.78, 'y': 6.76, 'z': 4.21\n",
        "} # expected 7703\n",
        "\n",
        "predicted_price = predict_diamond_price(sample_diamond, loaded_model, loaded_preprocessor)\n",
        "print(f\"Diamond Attributes: {sample_diamond}\")\n",
        "print(f\"Predicted Price: ${predicted_price:,.2f}\")"
      ],
      "metadata": {
        "id": "BfWqOy0zZl4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}